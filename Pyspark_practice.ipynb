{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7a7KCBt8lCfyu6bmIVKxl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sudhamsh1/Data_Analytics/blob/main/Pyspark_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_fmv3hVPVjp",
        "outputId": "47d938a9-7124-4774-d530-e05c57629ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from Pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: Pyspark\n",
            "  Building wheel for Pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=0f566ee79b169c62c443aa534346c56ed23c97dd11323542a5da605190d21de1\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built Pyspark\n",
            "Installing collected packages: Pyspark\n",
            "Successfully installed Pyspark-3.5.3\n"
          ]
        }
      ],
      "source": [
        "pip install Pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"Practice\").getOrCreate()\n",
        "\n"
      ],
      "metadata": {
        "id": "CHFakHd8VbjF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=[[1,5],[1,5],[1,5],[2,6],[1,5]]\n",
        "df=spark.createDataFrame(data,[\"a\",\"b\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-PNDeydVbwm",
        "outputId": "de550909-8143-480e-c144-0eb6ce3dee5a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "|  a|  b|\n",
            "+---+---+\n",
            "|  1|  5|\n",
            "|  1|  5|\n",
            "|  1|  5|\n",
            "|  2|  6|\n",
            "|  1|  5|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_1=df.filter(df.a==1)\n",
        "df_1=df_1.agg(sum(df_1.b))\n",
        "df_1.show()\n",
        "\n",
        "df_2=df.withColumn(\"c\",when(df.a==1,20).otherwise(df.b))\n",
        "\n",
        "df_2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8ND0qJXVbyI",
        "outputId": "54ad590a-bf83-4aa0-d15a-3c20261269b8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|sum(b)|\n",
            "+------+\n",
            "|    20|\n",
            "+------+\n",
            "\n",
            "+---+---+---+\n",
            "|  a|  b|  c|\n",
            "+---+---+---+\n",
            "|  1|  5| 20|\n",
            "|  1|  5| 20|\n",
            "|  1|  5| 20|\n",
            "|  2|  6|  6|\n",
            "|  1|  5| 20|\n",
            "+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1#)Create a Dataframe from existing one in such a way that all the columns should have single valued attribute\n",
        "#1NF\n",
        "\n",
        "data=(['Alice',['Tennis','batminton']],['bob',['Tennis','Cricket']],['Julie',['Cricket','Carroms']])\n",
        "\n",
        "df1=spark.createDataFrame(data,['name','hobbies'])\n",
        "\n",
        "df1.select(df1.name,explode(df1.hobbies).alias('hobbies')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya6PmClIVb0S",
        "outputId": "24aae5ad-28a3-4ac2-fab9-3557d9f6e1eb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+\n",
            "| name|  hobbies|\n",
            "+-----+---------+\n",
            "|Alice|   Tennis|\n",
            "|Alice|batminton|\n",
            "|  bob|   Tennis|\n",
            "|  bob|  Cricket|\n",
            "|Julie|  Cricket|\n",
            "|Julie|  Carroms|\n",
            "+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WDBzc2x7CrbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=[('Goa', '', 'AP'), ('', 'AP', None), (None, '', 'Blr')]\n",
        "schema=\"City1 string, City2 string, City3 string\"\n",
        "df2=spark.createDataFrame(data, schema)\n",
        "df2.show()\n",
        "df3=df2.withColumn('city1',when(df2.City1==\"\",None).otherwise(df2.City1))\\\n",
        "       .withColumn('city2',when(df2.City2==\"\",None).otherwise(df2.City2))\\\n",
        "       .withColumn('city3',when(df2.City3==\"\",None).otherwise(df2.City3))\n",
        "\n",
        "df3.withColumn(\"Result\",coalesce(df3.city1,df3.city2,df3.city3)).show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYVudHp2Vb28",
        "outputId": "b664b425-91b1-4cc2-ae13-b54c175cf6ae"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+\n",
            "|City1|City2|City3|\n",
            "+-----+-----+-----+\n",
            "|  Goa|     |   AP|\n",
            "|     |   AP| NULL|\n",
            "| NULL|     |  Blr|\n",
            "+-----+-----+-----+\n",
            "\n",
            "+-----+-----+-----+------+\n",
            "|city1|city2|city3|Result|\n",
            "+-----+-----+-----+------+\n",
            "|  Goa| NULL|   AP|   Goa|\n",
            "| NULL|   AP| NULL|    AP|\n",
            "| NULL| NULL|  Blr|   Blr|\n",
            "+-----+-----+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[('Goa', '', 'AP'), ('', 'AP', None), (None, '', 'Blr')]\n",
        "schema=\"City1 string, City2 string, City3 string\"\n",
        "df2=spark.createDataFrame(data, schema)\n",
        "df2.show()\n",
        "df3=df2.withColumn(\"City1\",when(df2.City1==\"\",None).otherwise(df2.City1))\\\n",
        "   .withColumn(\"City2\",when(df2.City2==\"\",None).otherwise(df2.City2))\\\n",
        "   .withColumn(\"City3\",when(df2.City3==\"\",None).otherwise(df2.City3))\n",
        "df3.show()\n",
        "\n",
        "df3.withColumn(\"Result\",coalesce(df3.City1,df3.City2,df3.City3)).select('Result').show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL2lBZyvVb50",
        "outputId": "7c3c3741-31ce-460e-ac57-bb112cef3572"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+\n",
            "|City1|City2|City3|\n",
            "+-----+-----+-----+\n",
            "|  Goa|     |   AP|\n",
            "|     |   AP| NULL|\n",
            "| NULL|     |  Blr|\n",
            "+-----+-----+-----+\n",
            "\n",
            "+-----+-----+-----+\n",
            "|City1|City2|City3|\n",
            "+-----+-----+-----+\n",
            "|  Goa| NULL|   AP|\n",
            "| NULL|   AP| NULL|\n",
            "| NULL| NULL|  Blr|\n",
            "+-----+-----+-----+\n",
            "\n",
            "+------+\n",
            "|Result|\n",
            "+------+\n",
            "|   Goa|\n",
            "|    AP|\n",
            "|   Blr|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=(['Alice','Tennis,batminton'],['bob','Tennis,Cricket'],['Julie','Cricket,Carroms'])\n",
        "\n",
        "df1=spark.createDataFrame(data,['name','hobbies'])\n",
        "\n",
        "df=df1.select(df1.name,split(df1.hobbies,',').alias('hobbies'))\n",
        "df.select(df.name,explode(df.hobbies).alias('hobbies')).show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAJXtIdIVb8h",
        "outputId": "bd0ba191-5a49-401b-8d66-01e05552ee76"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+\n",
            "| name|  hobbies|\n",
            "+-----+---------+\n",
            "|Alice|   Tennis|\n",
            "|Alice|batminton|\n",
            "|  bob|   Tennis|\n",
            "|  bob|  Cricket|\n",
            "|Julie|  Cricket|\n",
            "|Julie|  Carroms|\n",
            "+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Alternate Approach (Can be ignored)\n",
        "data=[('Goa', '', 'AP'), ('', 'AP', None), (None, '', 'Blr')]\n",
        "schema=\"City1 string, City2 string, City3 string\"\n",
        "df2=spark.createDataFrame(data, schema)\n",
        "\n",
        "df2.withColumn(\"Result\",coalesce(when(df2.City1==\"\",None).otherwise(df2.City1),(when(df2.City2==\"\",None).otherwise(df2.City2)),when(df2.City3==\"\",None).otherwise(df2.City3))).select('Result').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-57yWtT3Vb_O",
        "outputId": "17b698e6-1d7c-4635-de24-1e5763c935a3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|Result|\n",
            "+------+\n",
            "|   Goa|\n",
            "|    AP|\n",
            "|   Blr|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[[1,'Steve','Maths',90],[1,'Steve','Science',100],[2,'Philips','Maths',90],[2,'Philips','Science',90]]\n",
        "\n",
        "df=spark.createDataFrame(data,['ID','Name','Subject','Marks'])\n",
        "df.show()\n",
        "d=df.groupBy('ID','Name').agg(((sum(df.Marks))/count('*')).alias('Average'))\n",
        "\n",
        "d.withColumn(\"Grade\",when(d.Average>=90,'Distinction').when(d.Average>80,'First Class').otherwise('Fail')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSoEblMnVcCC",
        "outputId": "fdb0e842-9904-453f-ce91-f393e4cc3af3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-------+-----+\n",
            "| ID|   Name|Subject|Marks|\n",
            "+---+-------+-------+-----+\n",
            "|  1|  Steve|  Maths|   90|\n",
            "|  1|  Steve|Science|  100|\n",
            "|  2|Philips|  Maths|   90|\n",
            "|  2|Philips|Science|   90|\n",
            "+---+-------+-------+-----+\n",
            "\n",
            "+---+-------+-------+-----------+\n",
            "| ID|   Name|Average|      Grade|\n",
            "+---+-------+-------+-----------+\n",
            "|  1|  Steve|   95.0|Distinction|\n",
            "|  2|Philips|   90.0|Distinction|\n",
            "+---+-------+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "data=[[1,'Sudhamsh','Data',80000],[2,'Tanvi','Automation',81000],[3,'Varma','Data',75000],[4,'Nibbi','Automation',82000]]\n",
        "\n",
        "df=spark.createDataFrame(data,['ID','Name','Department','Salary'])\n",
        "w=Window.partitionBy(df.Department).orderBy(df['Salary'].desc)\n",
        "df.withColumn('Rank',rank().over(w)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Agj2MOt2aPb5",
        "outputId": "c4a97a6e-1737-4e4e-8376-7acb71de688a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PySparkTypeError",
          "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got method.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-ffaea71a7c2d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Department'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Salary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDepartment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Salary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rank'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWindowSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFuncT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/window.py\u001b[0m in \u001b[0;36morderBy\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mnames\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \"\"\"\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mWindowSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_java_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtry_remote_windowspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/window.py\u001b[0m in \u001b[0;36m_to_java_cols\u001b[0;34m(cols)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_active_spark_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ColumnOrName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         raise PySparkTypeError(\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NOT_COLUMN_OR_STR\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got method."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r4ytuZ-KcWZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data = [\n",
        "('Product_A', 100, 150, 200),\n",
        "('Product_B', 120, 80, 110),\n",
        "('Product_C', 90, 130, 180),\n",
        "('Product_D', 200, 160, 120)\n",
        "]\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(sales_data,['Product','Jan_Sales','Feb_Sales','Mar_Sales'])\n",
        "df.show()\n",
        "updf=df.unpivot('Product',['Jan_Sales','Feb_sales','Mar_sales'],'Month','Sales')\n",
        "updf.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "7MDfca5saPep",
        "outputId": "e8370807-7529-45b3-f6c8-87a1704a24c2"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+---------+---------+\n",
            "|  Product|Jan_Sales|Feb_Sales|Mar_Sales|\n",
            "+---------+---------+---------+---------+\n",
            "|Product_A|      100|      150|      200|\n",
            "|Product_B|      120|       80|      110|\n",
            "|Product_C|       90|      130|      180|\n",
            "|Product_D|      200|      160|      120|\n",
            "+---------+---------+---------+---------+\n",
            "\n",
            "+---------+---------+-----+\n",
            "|  Product|    Month|Sales|\n",
            "+---------+---------+-----+\n",
            "|Product_A|Jan_Sales|  100|\n",
            "|Product_A|Feb_sales|  150|\n",
            "|Product_A|Mar_sales|  200|\n",
            "|Product_B|Jan_Sales|  120|\n",
            "|Product_B|Feb_sales|   80|\n",
            "|Product_B|Mar_sales|  110|\n",
            "|Product_C|Jan_Sales|   90|\n",
            "|Product_C|Feb_sales|  130|\n",
            "|Product_C|Mar_sales|  180|\n",
            "|Product_D|Jan_Sales|  200|\n",
            "|Product_D|Feb_sales|  160|\n",
            "|Product_D|Mar_sales|  120|\n",
            "+---------+---------+-----+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'pivot'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-432e4848335a>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mupdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Product'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Jan_Sales'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Feb_sales'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Mar_sales'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Month'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Sales'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mupdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mupdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Month'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Sales'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3127\u001b[0m         \"\"\"\n\u001b[1;32m   3128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3129\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   3130\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m             )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'pivot'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rp4dzL1Mti-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lO4lAkN7aPhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rrLLYTZaPkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nj-kII2raPnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hN3eDLGvaPqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_nNkPKXaPt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NESV2FRLaPxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RdNihTojaPyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GqEeCDXsaP15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SvYw6f7mVcFi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}